{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9Rny-N8GuEU"
      },
      "source": [
        "# Assignment 7: Dimensionality Reduction\n",
        "\n",
        "## Follow These Steps Before Submitting\n",
        "Once you are finished, ensure to complete the following steps.\n",
        "\n",
        "1.  Restart your kernel by clicking **'Runtime' > 'Restart session and run all'**.\n",
        "\n",
        "2.  Fix any errors which result from this.\n",
        "\n",
        "3.  Repeat steps 1. and 2. until your notebook runs without errors.\n",
        "\n",
        "4.  Submit your completed notebook to OWL by the deadline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwjPDT81Gzl7"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "In this assignment, you will work on a text dataset. The Yelp reviews dataset consists of reviews from Yelp. It is extracted from the Yelp Dataset Challenge 2015 data. For more information, please refer to http://www.yelp.com/dataset_challenge. The Yelp reviews polarity dataset is a subset of Yelp reviews dataset and is constructed by considering stars 1 and 2 negative, and 3 and 4 positive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUdNJyxCGttn"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "from scipy.sparse import csr_matrix\n",
        "import sklearn.feature_extraction.text as sktext\n",
        "from sklearn.decomposition import PCA, SparsePCA, TruncatedSVD\n",
        "import re\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import umap\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0T2v750Gfhh"
      },
      "outputs": [],
      "source": [
        "!gdown https://drive.google.com/uc?id=1A0-Q7SbdoA3r7aawraRSwMgKujBYlFLv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXEkCMHJTKZm"
      },
      "source": [
        "# Part 1: Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzDP3ECrTMp0"
      },
      "source": [
        "## Question 1.1: Load data\n",
        "\n",
        "Read the **`yelp.csv`** file as a **`polars.DataFrame`** and show the first 5 rows of the dataframe and its descriptive statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "_xTI4tysNhHf",
        "outputId": "603607de-7587-48c2-ab11-d5d198b0fea9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (5, 2)\n",
            "┌───────────┬─────────────────────────────────┐\n",
            "│ Sentiment ┆ Review                          │\n",
            "│ ---       ┆ ---                             │\n",
            "│ i64       ┆ str                             │\n",
            "╞═══════════╪═════════════════════════════════╡\n",
            "│ 0         ┆ Maintenance here is ridiculous… │\n",
            "│ 1         ┆ I really enjoy smaller more in… │\n",
            "│ 1         ┆ Looking at their menu, I was a… │\n",
            "│ 1         ┆ Best sandwiches in Las Vegas! … │\n",
            "│ 0         ┆ Was upset because they didnt h… │\n",
            "└───────────┴─────────────────────────────────┘\n",
            "shape: (9, 3)\n",
            "┌────────────┬───────────┬─────────────────────────────────┐\n",
            "│ statistic  ┆ Sentiment ┆ Review                          │\n",
            "│ ---        ┆ ---       ┆ ---                             │\n",
            "│ str        ┆ f64       ┆ str                             │\n",
            "╞════════════╪═══════════╪═════════════════════════════════╡\n",
            "│ count      ┆ 1500.0    ┆ 1500                            │\n",
            "│ null_count ┆ 0.0       ┆ 0                               │\n",
            "│ mean       ┆ 0.494     ┆ null                            │\n",
            "│ std        ┆ 0.500131  ┆ null                            │\n",
            "│ min        ┆ 0.0       ┆ $24 for a burger and beer.  \\n… │\n",
            "│ 25%        ┆ 0.0       ┆ null                            │\n",
            "│ 50%        ┆ 0.0       ┆ null                            │\n",
            "│ 75%        ┆ 1.0       ┆ null                            │\n",
            "│ max        ┆ 1.0       ┆ wow, where do I begin?  It too… │\n",
            "└────────────┴───────────┴─────────────────────────────────┘\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "df = pl.read_csv(\"yelp.csv\")\n",
        "\n",
        "# Display the first 5 rows\n",
        "print(df.head(5))\n",
        "\n",
        "# Show descriptive statistics\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wYvWjR-OrMt"
      },
      "source": [
        "## Question 1.2: Convert categorical variable\n",
        "\n",
        "Since we are not predicting the categorical variable in this assignment, let's convert **`Sentiment`** to string:\n",
        "- Replace **1** with **`positive`**.\n",
        "- Replace **0** with **`negative`**.\n",
        "\n",
        "Display the first 5 rows of the resulting dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "suyRJ7aTOze_",
        "outputId": "a7ab0564-8f6c-4001-ff0c-7fdfc6eb538d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (5, 2)\n",
            "┌───────────┬─────────────────────────────────┐\n",
            "│ Sentiment ┆ Review                          │\n",
            "│ ---       ┆ ---                             │\n",
            "│ str       ┆ str                             │\n",
            "╞═══════════╪═════════════════════════════════╡\n",
            "│ negative  ┆ Maintenance here is ridiculous… │\n",
            "│ positive  ┆ I really enjoy smaller more in… │\n",
            "│ positive  ┆ Looking at their menu, I was a… │\n",
            "│ positive  ┆ Best sandwiches in Las Vegas! … │\n",
            "│ negative  ┆ Was upset because they didnt h… │\n",
            "└───────────┴─────────────────────────────────┘\n"
          ]
        }
      ],
      "source": [
        "# Convert Sentiment column\n",
        "df = df.with_columns(\n",
        "  df[\"Sentiment\"].cast(pl.Utf8) # Ensure it's a string type first\n",
        "  .replace(\"1\", \"positive\")\n",
        "  .replace(\"0\", \"negative\")\n",
        ")\n",
        "\n",
        "# Display the first 5 rows\n",
        "print(df.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1OKBpraT7F2"
      },
      "source": [
        "## Question 1.3: Transform text\n",
        "\n",
        "Apply **`Term Frequency - Inverse Document Frequency`** transformation using [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html):\n",
        "- Eliminate accents and other characters\n",
        "- Eliminate stopwords\n",
        "- Eliminate words that appear in less than 5% and words that appear in more than 95% of texts\n",
        "- Apply sublinear tf scaling\n",
        "\n",
        "Extract and save the word list. Report the number of words that are kept."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "6iWfkKp_U8T4",
        "outputId": "51a86d8d-8a2f-4f97-dfce-8a5a7da1751a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words kept: 157\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Extract text data\n",
        "texts = df[\"Review\"].to_list()\n",
        "\n",
        "# Initialize the TF-IDF Vectorizer with specified constraints\n",
        "vectorizer = TfidfVectorizer(\n",
        "  strip_accents=\"unicode\", # Normalize accents\n",
        "  stop_words=\"english\",    # Remove stopwords\n",
        "  min_df=0.05,             # Ignore words in <5% of documents\n",
        "  max_df=0.95,             # Ignroe words in >95% of documents\n",
        "  sublinear_tf=True        # Apply sublinear TF scaling\n",
        ")\n",
        "\n",
        "# Fit the vectorizer to the text data\n",
        "tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Get the feature names (words)\n",
        "word_list = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Report the number of words kept\n",
        "num_words_kept = len(word_list)\n",
        "print(f\"Number of words kept: {num_words_kept}\")\n",
        "\n",
        "# Save the word list\n",
        "with open(\"word_list.txt\", \"w\") as f:\n",
        "  for word in word_list:\n",
        "    f.write(word + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsfYvGN6YW00"
      },
      "source": [
        "## Question 1.4: Explore words\n",
        "\n",
        "Based on TF-IDF scores, show the 10 most often repeated words and the 10 least often repeated words.\n",
        "\n",
        "Hint: You might need to use [`np.argsort`](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html). Pay attention to sorting order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "0E_ZFt68WUyD",
        "outputId": "59f19d73-1095-47c6-ee3d-e16034282db6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The top 10 most often repeated words:\n",
            "food\n",
            "place\n",
            "good\n",
            "great\n",
            "service\n",
            "like\n",
            "just\n",
            "time\n",
            "really\n",
            "don\n",
            "\n",
            "The top 10 least often repeated words:\n",
            "having\n",
            "tell\n",
            "half\n",
            "decided\n",
            "town\n",
            "30\n",
            "finally\n",
            "kind\n",
            "review\n",
            "reviews\n"
          ]
        }
      ],
      "source": [
        "# Compute the mean TF-IDF score for each word across all reviews\n",
        "word_tfidf_scores = np.array(tfidf_matrix.mean(axis=0)).flatten()\n",
        "\n",
        "# Get the indices of the highest and lowest TF-IDF scores\n",
        "top_10_indices = np.argsort(word_tfidf_scores)[-10:][::-1] # 10 largest values (descending)\n",
        "bottom_10_indices = np.argsort(word_tfidf_scores)[:10] # 10 smallest valeus (ascending)\n",
        "\n",
        "# Retreive the corresponding words\n",
        "top_10_words = [word_list[i] for i in top_10_indices]\n",
        "bottom_10_words = [word_list[i] for i in bottom_10_indices]\n",
        "\n",
        "# Dispaly results\n",
        "print(\"The top 10 most often repeated words:\")\n",
        "for word in top_10_words:\n",
        "  print(word)\n",
        "\n",
        "print(\"\\nThe top 10 least often repeated words:\")\n",
        "for word in bottom_10_words:\n",
        "  print(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F5rFv9fbYm7"
      },
      "source": [
        "# Part 2: Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHWiRzbtb8B6"
      },
      "source": [
        "## Question 2.1: PCA\n",
        "\n",
        "(1) Apply **normal PCA**. Set the number of components to 100. Report the percentage variance explained by the 100 PCs.\n",
        "\n",
        "(2) Show the words that have positive weight in the **third PC** (index 2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ur_53Z-ZrdB"
      },
      "outputs": [],
      "source": [
        "# (1) YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSw838Ybi5IY"
      },
      "outputs": [],
      "source": [
        "# (2) YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYhkbxQwgY63"
      },
      "source": [
        "## Question 2.2: LSA\n",
        "\n",
        "(1) Apply **LSA** using [`TruncatedSVD`](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html). Set:\n",
        "- number of components to 100\n",
        "- number of iterations to 10\n",
        "- random state to 2025.\n",
        "\n",
        "Report the percentage variance explained by the 100 PCs.\n",
        "\n",
        "(2) Show the five words that relate the most with the **fifth PC** (index 4). What would you name this principal component?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1bHKJzVdm75"
      },
      "outputs": [],
      "source": [
        "# (1) YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcv37tGhmWRr"
      },
      "outputs": [],
      "source": [
        "# (2) YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WTz0HOCG0gE"
      },
      "source": [
        "**Written answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEv01D_RKnSc"
      },
      "source": [
        "## Question 2.3: PCA vs LSA\n",
        "\n",
        "Compare PCA and LSA. Comment on your findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE6bFhEAKyGQ"
      },
      "source": [
        "**Written answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LEHGcsdrE5U"
      },
      "source": [
        "## Question 2.4: t-SNE\n",
        "\n",
        "Apply **t-SNE**. Set:\n",
        "- number of components to 2\n",
        "- random first inintialization\n",
        "- try a perplexity of 2 and 10.\n",
        "- tightness of natural clusters to 30\n",
        "- auto learning rate\n",
        "- maximum number of iterations to 1000\n",
        "- maximum number of iterations without progress before we abort to 100\n",
        "- use cosine metric\n",
        "- gradient threshold to 0.0000001\n",
        "- random state to 2025\n",
        "\n",
        "Create a plot, showing 2D projection of our data using t-SNE for both perplexities, in separate plots. Remember to add labels and title.\n",
        "\n",
        "Written answer: Compare the two projections. Which projection would you think separates the classes better? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaCOQop3tFWF"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE - Perplexity 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i88VKrCTvTq8"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE - Perplexity 10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHKGIDfpG0gF"
      },
      "source": [
        "**Written Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6KQro2bvYzN"
      },
      "source": [
        "## Question 2.5: UMAP\n",
        "\n",
        "(1) Apply **UMAP**. Set:\n",
        "- number of components to 2\n",
        "- use 10 nearest neighbors\n",
        "- use cosine metric\n",
        "- number of training epochs to 1000\n",
        "- effective minimum distance between embedded points to 0.1\n",
        "- effective scale of embedded points to 1\n",
        "- avoids excessive memory use\n",
        "- do not use a random seed to allow parallel processing.\n",
        "\n",
        "(2) Create a plot, showing 2D projection of our data using UMAP. Remember to add labels and title."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkBuHmJ2rTqh"
      },
      "outputs": [],
      "source": [
        "# (1) YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60_w0xPuwrn5"
      },
      "outputs": [],
      "source": [
        "# (2) YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrDtBc49Lam9"
      },
      "source": [
        "## Question 2.6: t-SNE vs UMAP\n",
        "\n",
        "Compare t-SNE (perplexity 10) and UMAP. Comment on your findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofbHPl-LLbVq"
      },
      "source": [
        "**Written answer:**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}