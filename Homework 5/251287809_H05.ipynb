{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LJCGYX0iRh9j",
        "UMaaQ19-Xd1d",
        "Yv9TQIWqoWpY",
        "61faU0aZzRqk",
        "766tNRJzbBFA",
        "mxDVkMQ-hEem",
        "6MKT3NRarkz5",
        "ACAsAnUdLB_o"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emilyberlinghoff/DS3000/blob/main/Homework%205/251287809_H05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 5: Logistic Regression with Variable Selection\n",
        "\n",
        "## Follow These Steps Before Submitting\n",
        "Once you are finished, ensure to complete the following steps.\n",
        "\n",
        "1.  Restart your kernel by clicking **'Runtime' > 'Restart session and run all'**.\n",
        "\n",
        "2.  Fix any errors which result from this.\n",
        "\n",
        "3.  Repeat steps 1. and 2. until your notebook runs without errors.\n",
        "\n",
        "4.  Submit your completed notebook to OWL by the deadline."
      ],
      "metadata": {
        "id": "m9l0b9ZnSgPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "\n",
        "In this assignment, you will work on a dataset taken from USGS(U.S Geological Survey). This dataset contains earthquake data with a magnitude of 4.5+ and an \"alert\" warning level, recorded between 1976 and 2025. Below is an explanation of the columns included in the dataset:\n",
        "\n",
        "- **`id`**: A unique identifier for the earthquake event.\n",
        "- **`time`**: The timestamp indicating when the earthquake or event occurred, including the date and time in UTC format.\n",
        "- **`latitude`**: The geographical latitude of the earthquake's epicenter, measured in degrees.\n",
        "- **`longitude`**: The geographical longitude of the earthquake's epicenter, measured in degrees.\n",
        "- **`depth`**: The depth at which the earthquake occurred, typically measured in kilometers below the Earth's surface.\n",
        "- **`mag`**: The magnitude of the earthquake, representing the energy released by the seismic event. In this case, a value of 8.6 indicates a very large earthquake.\n",
        "- **`gap`**: The azimuthal gap, which refers to the angular distance between the two most distant seismic stations that recorded the earthquake. A smaller gap typically indicates better global coverage.\n",
        "- **`dmin`**: The minimum distance between the earthquake's epicenter and the nearest seismic station, measured in degrees.\n",
        "- **`rms`**: The root mean square of the amplitude of the seismic waves, representing the strength of the seismic signal.\n",
        "- **`horizontalError`**: The error associated with the latitude and longitude coordinates of the epicenter, typically measured in kilometers.\n",
        "- **`depthError`**: The error associated with the depth measurement of the earthquake, typically measured in kilometers.\n",
        "- **`magError`**: The error associated with the magnitude measurement of the earthquake, representing the uncertainty in the reported magnitude.\n",
        "- **`magNst`**: The number of stations that contributed to the magnitude estimation.\n",
        "- **`Alert` (target)** The alert level issued for the earthquake, whether 'Severe' or 'Non-Severe'.\n",
        "\n",
        "The goal is to train a model for predicting the **`Alert`** which indicates the severity of the earthquake.\n",
        "\n"
      ],
      "metadata": {
        "id": "LJCGYX0iRh9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard imports\n",
        "import numpy as np\n",
        "from itertools import chain, combinations\n",
        "\n",
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "\n",
        "# Sklearn imports\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "\n",
        "# Plotting packages\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "zoXM0sRHQne_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the data\n",
        "!gdown https://drive.google.com/uc?id=1yL84FMQrfHC_cQsa_V3KTcRAJS0k4DhY"
      ],
      "metadata": {
        "id": "dIBHhqMZUx3R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed369aab-45bf-47a9-893a-2f95ab07ab3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1yL84FMQrfHC_cQsa_V3KTcRAJS0k4DhY\n",
            "To: /content/earthquakes.parquet\n",
            "\r  0% 0.00/437k [00:00<?, ?B/s]\r100% 437k/437k [00:00<00:00, 34.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Data Preprocessing"
      ],
      "metadata": {
        "id": "l8G0CQ-bXMMM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1.1: Load data\n",
        "\n",
        "(1) Read the **`earthquakes.parquet`** file as a **`polars.DataFrame`** and show its descriptive statistics.\n",
        "\n",
        "(2) Drop column **`id`** and **`time`** and display the first 5 rows of the dataframe.\n",
        "\n",
        "Since **`id`** is unique for each earthquake event that does not contain any predictive information and **`time`** is not directly informative for predicting earthquake severity unless you extract relevant features such as time of day, seasonality, etc."
      ],
      "metadata": {
        "id": "UMaaQ19-Xd1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"earthquakes.parquet\"\n",
        "df = pl.read_parquet(file_path)\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "B6_VKNYOWLnI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "a633d960-c13f-4ada-fb92-d42183056d6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (9, 15)\n",
              "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬──────────┬───────────┬───────────┐\n",
              "│ statistic ┆ id        ┆ time      ┆ latitude  ┆ … ┆ depthErro ┆ magError ┆ magNst    ┆ Alert     │\n",
              "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ r         ┆ ---      ┆ ---       ┆ ---       │\n",
              "│ str       ┆ str       ┆ str       ┆ f64       ┆   ┆ ---       ┆ f64      ┆ f64       ┆ str       │\n",
              "│           ┆           ┆           ┆           ┆   ┆ f64       ┆          ┆           ┆           │\n",
              "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪══════════╪═══════════╪═══════════╡\n",
              "│ count     ┆ 7699      ┆ 7699      ┆ 7699.0    ┆ … ┆ 7490.0    ┆ 5485.0   ┆ 5632.0    ┆ 7699      │\n",
              "│ null_coun ┆ 0         ┆ 0         ┆ 0.0       ┆ … ┆ 209.0     ┆ 2214.0   ┆ 2067.0    ┆ 0         │\n",
              "│ t         ┆           ┆           ┆           ┆   ┆           ┆          ┆           ┆           │\n",
              "│ mean      ┆ null      ┆ null      ┆ 0.999312  ┆ … ┆ 2.571899  ┆ 0.06792  ┆ 46.712713 ┆ null      │\n",
              "│ std       ┆ null      ┆ null      ┆ 32.359887 ┆ … ┆ 3.119655  ┆ 0.057433 ┆ 69.220344 ┆ null      │\n",
              "│ min       ┆ ak012fko1 ┆ 1976-03-2 ┆ -69.7739  ┆ … ┆ 0.0       ┆ 0.0      ┆ 0.0       ┆ Non-Sever │\n",
              "│           ┆ 6th       ┆ 5 00:41:2 ┆           ┆   ┆           ┆          ┆           ┆ e         │\n",
              "│           ┆           ┆ 0.500000+ ┆           ┆   ┆           ┆          ┆           ┆           │\n",
              "│           ┆           ┆ 00:…      ┆           ┆   ┆           ┆          ┆           ┆           │\n",
              "│ 25%       ┆ null      ┆ null      ┆ -21.7389  ┆ … ┆ 1.7       ┆ 0.047    ┆ 18.0      ┆ null      │\n",
              "│ 50%       ┆ null      ┆ null      ┆ -3.5114   ┆ … ┆ 1.8       ┆ 0.059    ┆ 29.0      ┆ null      │\n",
              "│ 75%       ┆ null      ┆ null      ┆ 28.2943   ┆ … ┆ 2.9       ┆ 0.073    ┆ 47.0      ┆ null      │\n",
              "│ max       ┆ uw6156212 ┆ 2024-12-2 ┆ 85.729    ┆ … ┆ 31.95     ┆ 1.642    ┆ 954.0     ┆ Severe    │\n",
              "│           ┆ 6         ┆ 3 06:00:5 ┆           ┆   ┆           ┆          ┆           ┆           │\n",
              "│           ┆           ┆ 9.074000+ ┆           ┆   ┆           ┆          ┆           ┆           │\n",
              "│           ┆           ┆ 00:…      ┆           ┆   ┆           ┆          ┆           ┆           │\n",
              "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴──────────┴───────────┴───────────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (9, 15)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>id</th><th>time</th><th>latitude</th><th>longitude</th><th>depth</th><th>mag</th><th>gap</th><th>dmin</th><th>rms</th><th>horizontalError</th><th>depthError</th><th>magError</th><th>magNst</th><th>Alert</th></tr><tr><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>&quot;7699&quot;</td><td>&quot;7699&quot;</td><td>7699.0</td><td>7699.0</td><td>7699.0</td><td>7699.0</td><td>7375.0</td><td>6997.0</td><td>7673.0</td><td>6481.0</td><td>7490.0</td><td>5485.0</td><td>5632.0</td><td>&quot;7699&quot;</td></tr><tr><td>&quot;null_count&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>324.0</td><td>702.0</td><td>26.0</td><td>1218.0</td><td>209.0</td><td>2214.0</td><td>2067.0</td><td>&quot;0&quot;</td></tr><tr><td>&quot;mean&quot;</td><td>null</td><td>null</td><td>0.999312</td><td>15.173904</td><td>58.990024</td><td>5.657358</td><td>49.060709</td><td>4.109992</td><td>0.822303</td><td>6.974621</td><td>2.571899</td><td>0.06792</td><td>46.712713</td><td>null</td></tr><tr><td>&quot;std&quot;</td><td>null</td><td>null</td><td>32.359887</td><td>128.333755</td><td>119.160114</td><td>0.514095</td><td>39.120351</td><td>5.259451</td><td>0.266227</td><td>2.806992</td><td>3.119655</td><td>0.057433</td><td>69.220344</td><td>null</td></tr><tr><td>&quot;min&quot;</td><td>&quot;ak012fko16th&quot;</td><td>&quot;1976-03-25 00:41:20.500000+00:…</td><td>-69.7739</td><td>-179.9776</td><td>-1.77</td><td>4.5</td><td>7.0</td><td>0.0</td><td>0.04</td><td>0.08</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&quot;Non-Severe&quot;</td></tr><tr><td>&quot;25%&quot;</td><td>null</td><td>null</td><td>-21.7389</td><td>-109.6226</td><td>10.0</td><td>5.4</td><td>25.0</td><td>1.047</td><td>0.66</td><td>5.6</td><td>1.7</td><td>0.047</td><td>18.0</td><td>null</td></tr><tr><td>&quot;50%&quot;</td><td>null</td><td>null</td><td>-3.5114</td><td>42.1877</td><td>13.7</td><td>5.6</td><td>38.0</td><td>2.407</td><td>0.82</td><td>7.1</td><td>1.8</td><td>0.059</td><td>29.0</td><td>null</td></tr><tr><td>&quot;75%&quot;</td><td>null</td><td>null</td><td>28.2943</td><td>141.2441</td><td>42.0</td><td>5.9</td><td>59.0</td><td>4.969</td><td>0.99</td><td>8.6</td><td>2.9</td><td>0.073</td><td>47.0</td><td>null</td></tr><tr><td>&quot;max&quot;</td><td>&quot;uw61562126&quot;</td><td>&quot;2024-12-23 06:00:59.074000+00:…</td><td>85.729</td><td>179.9981</td><td>670.81</td><td>8.6</td><td>321.0</td><td>39.934</td><td>2.1</td><td>88.54</td><td>31.95</td><td>1.642</td><td>954.0</td><td>&quot;Severe&quot;</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop([\"id\", \"time\"])\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "6VQganpcnIEZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "546174dd-3018-4177-9217-e91609d853a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (5, 12)\n",
              "┌───────────┬─────────────┬────────┬──────┬───┬────────────┬──────────┬────────┬────────────┐\n",
              "│ latitude  ┆ longitude   ┆ depth  ┆ mag  ┆ … ┆ depthError ┆ magError ┆ magNst ┆ Alert      │\n",
              "│ ---       ┆ ---         ┆ ---    ┆ ---  ┆   ┆ ---        ┆ ---      ┆ ---    ┆ ---        │\n",
              "│ f64       ┆ f64         ┆ f64    ┆ f64  ┆   ┆ f64        ┆ f64      ┆ f64    ┆ str        │\n",
              "╞═══════════╪═════════════╪════════╪══════╪═══╪════════════╪══════════╪════════╪════════════╡\n",
              "│ 35.59     ┆ -90.48      ┆ 15.0   ┆ 4.62 ┆ … ┆ null       ┆ null     ┆ 0.0    ┆ Non-Severe │\n",
              "│ 32.998667 ┆ -115.5575   ┆ 14.19  ┆ 5.8  ┆ … ┆ 1.78       ┆ null     ┆ 0.0    ┆ Severe     │\n",
              "│ 38.19     ┆ -83.95      ┆ 10.0   ┆ 5.0  ┆ … ┆ null       ┆ null     ┆ null   ┆ Non-Severe │\n",
              "│ 35.816    ┆ -117.816333 ┆ 4.766  ┆ 4.7  ┆ … ┆ 31.61      ┆ 0.424    ┆ 9.0    ┆ Non-Severe │\n",
              "│ 33.0955   ┆ -115.6245   ┆ 18.904 ┆ 5.75 ┆ … ┆ 0.67       ┆ 0.161    ┆ 6.0    ┆ Non-Severe │\n",
              "└───────────┴─────────────┴────────┴──────┴───┴────────────┴──────────┴────────┴────────────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (5, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>latitude</th><th>longitude</th><th>depth</th><th>mag</th><th>gap</th><th>dmin</th><th>rms</th><th>horizontalError</th><th>depthError</th><th>magError</th><th>magNst</th><th>Alert</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td></tr></thead><tbody><tr><td>35.59</td><td>-90.48</td><td>15.0</td><td>4.62</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.0</td><td>&quot;Non-Severe&quot;</td></tr><tr><td>32.998667</td><td>-115.5575</td><td>14.19</td><td>5.8</td><td>79.0</td><td>0.05768</td><td>0.17</td><td>1.03</td><td>1.78</td><td>null</td><td>0.0</td><td>&quot;Severe&quot;</td></tr><tr><td>38.19</td><td>-83.95</td><td>10.0</td><td>5.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;Non-Severe&quot;</td></tr><tr><td>35.816</td><td>-117.816333</td><td>4.766</td><td>4.7</td><td>135.0</td><td>null</td><td>0.66</td><td>2.51</td><td>31.61</td><td>0.424</td><td>9.0</td><td>&quot;Non-Severe&quot;</td></tr><tr><td>33.0955</td><td>-115.6245</td><td>18.904</td><td>5.75</td><td>34.0</td><td>null</td><td>0.34</td><td>0.56</td><td>0.67</td><td>0.161</td><td>6.0</td><td>&quot;Non-Severe&quot;</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1.2: Handle null values\n",
        "\n",
        "The result of the `null_count` function indicates that some columns contain null values. Fill these null values with the median of the corresponding column and display the first 5 rows of the resulting dataframe."
      ],
      "metadata": {
        "id": "Yv9TQIWqoWpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.null_count() # uncomment and run this code"
      ],
      "metadata": {
        "id": "5SkBkhO7ouYE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "f9649563-56ec-4203-a139-777efbce1170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (1, 12)\n",
              "┌──────────┬───────────┬───────┬─────┬───┬────────────┬──────────┬────────┬───────┐\n",
              "│ latitude ┆ longitude ┆ depth ┆ mag ┆ … ┆ depthError ┆ magError ┆ magNst ┆ Alert │\n",
              "│ ---      ┆ ---       ┆ ---   ┆ --- ┆   ┆ ---        ┆ ---      ┆ ---    ┆ ---   │\n",
              "│ u32      ┆ u32       ┆ u32   ┆ u32 ┆   ┆ u32        ┆ u32      ┆ u32    ┆ u32   │\n",
              "╞══════════╪═══════════╪═══════╪═════╪═══╪════════════╪══════════╪════════╪═══════╡\n",
              "│ 0        ┆ 0         ┆ 0     ┆ 0   ┆ … ┆ 209        ┆ 2214     ┆ 2067   ┆ 0     │\n",
              "└──────────┴───────────┴───────┴─────┴───┴────────────┴──────────┴────────┴───────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (1, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>latitude</th><th>longitude</th><th>depth</th><th>mag</th><th>gap</th><th>dmin</th><th>rms</th><th>horizontalError</th><th>depthError</th><th>magError</th><th>magNst</th><th>Alert</th></tr><tr><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>324</td><td>702</td><td>26</td><td>1218</td><td>209</td><td>2214</td><td>2067</td><td>0</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filled_columns = []\n",
        "for col in df.columns:\n",
        "    median_value = df[col].median()\n",
        "    if median_value is not None:  # Ensure median value is valid\n",
        "        filled_columns.append(df[col].fill_null(median_value).alias(col))\n",
        "    else:\n",
        "        filled_columns.append(df[col])  # Keep column unchanged if median is None\n",
        "\n",
        "df = df.with_columns(filled_columns)\n",
        "print(df.head(5))"
      ],
      "metadata": {
        "id": "YvX9BKCnovfk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cde6babf-d582-4115-9f58-0a50d7b081f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (5, 12)\n",
            "┌───────────┬─────────────┬────────┬──────┬───┬────────────┬──────────┬────────┬────────────┐\n",
            "│ latitude  ┆ longitude   ┆ depth  ┆ mag  ┆ … ┆ depthError ┆ magError ┆ magNst ┆ Alert      │\n",
            "│ ---       ┆ ---         ┆ ---    ┆ ---  ┆   ┆ ---        ┆ ---      ┆ ---    ┆ ---        │\n",
            "│ f64       ┆ f64         ┆ f64    ┆ f64  ┆   ┆ f64        ┆ f64      ┆ f64    ┆ str        │\n",
            "╞═══════════╪═════════════╪════════╪══════╪═══╪════════════╪══════════╪════════╪════════════╡\n",
            "│ 35.59     ┆ -90.48      ┆ 15.0   ┆ 4.62 ┆ … ┆ 1.8        ┆ 0.059    ┆ 0.0    ┆ Non-Severe │\n",
            "│ 32.998667 ┆ -115.5575   ┆ 14.19  ┆ 5.8  ┆ … ┆ 1.78       ┆ 0.059    ┆ 0.0    ┆ Severe     │\n",
            "│ 38.19     ┆ -83.95      ┆ 10.0   ┆ 5.0  ┆ … ┆ 1.8        ┆ 0.059    ┆ 29.0   ┆ Non-Severe │\n",
            "│ 35.816    ┆ -117.816333 ┆ 4.766  ┆ 4.7  ┆ … ┆ 31.61      ┆ 0.424    ┆ 9.0    ┆ Non-Severe │\n",
            "│ 33.0955   ┆ -115.6245   ┆ 18.904 ┆ 5.75 ┆ … ┆ 0.67       ┆ 0.161    ┆ 6.0    ┆ Non-Severe │\n",
            "└───────────┴─────────────┴────────┴──────┴───┴────────────┴──────────┴────────┴────────────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1.3: Explore target distribution\n",
        "\n",
        "Count the number of instances of each severity level of the earthquake in the dataset.\n",
        "\n",
        "Comment on your findings, providing insights into the distribution of different severity levels."
      ],
      "metadata": {
        "id": "61faU0aZzRqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "severity_counts = df[\"Alert\"].value_counts()\n",
        "print(severity_counts)"
      ],
      "metadata": {
        "id": "8T3pp1W0zRDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8180d975-e59f-4fd1-f869-8e65bbf6cf5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (2, 2)\n",
            "┌────────────┬───────┐\n",
            "│ Alert      ┆ count │\n",
            "│ ---        ┆ ---   │\n",
            "│ str        ┆ u32   │\n",
            "╞════════════╪═══════╡\n",
            "│ Severe     ┆ 331   │\n",
            "│ Non-Severe ┆ 7368  │\n",
            "└────────────┴───────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset reveals a significant imbalance in the distribution of earthquake severity levels, with 7,368 instances of Non-Severe earthquakes compared to only 331 occurrences of Severe earthquakes. This suggests that severe earthquakes are relatively rare, comprising only a small fraction of recorded seismic events. The overwhelming prevalence of non-severe earthquakes indicates that most seismic activities in the dataset are of lower intensity."
      ],
      "metadata": {
        "id": "ZtiF3a-qzpPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1.4: Convert target variable\n",
        "\n",
        "Convert **`Alert`** to a binary numerical target:\n",
        "- Replace **`Severe`** with 1.\n",
        "- Replace **`Non-Severe`** with 0.\n",
        "\n",
        "Display the first 5 rows of the resulting dataframe.\n",
        "\n",
        "Hint: If you use the `replace` method, the resulting column will still be of string type. Use `cast` to make it `Float64` after replacement."
      ],
      "metadata": {
        "id": "766tNRJzbBFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.with_columns(\n",
        "    df[\"Alert\"]\n",
        "    .replace({\"Severe\": 1, \"Non-Severe\": 0})  # Replace values\n",
        "    .cast(pl.Float64)  # Cast to Float64\n",
        "    .alias(\"Alert\")  # Ensure column name remains the same\n",
        ")\n",
        "print(df.head(5))"
      ],
      "metadata": {
        "id": "qjO3iAxvaTpT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3fe34ac-d44a-49f6-c636-40cbb9987536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (5, 12)\n",
            "┌───────────┬─────────────┬────────┬──────┬───┬────────────┬──────────┬────────┬───────┐\n",
            "│ latitude  ┆ longitude   ┆ depth  ┆ mag  ┆ … ┆ depthError ┆ magError ┆ magNst ┆ Alert │\n",
            "│ ---       ┆ ---         ┆ ---    ┆ ---  ┆   ┆ ---        ┆ ---      ┆ ---    ┆ ---   │\n",
            "│ f64       ┆ f64         ┆ f64    ┆ f64  ┆   ┆ f64        ┆ f64      ┆ f64    ┆ f64   │\n",
            "╞═══════════╪═════════════╪════════╪══════╪═══╪════════════╪══════════╪════════╪═══════╡\n",
            "│ 35.59     ┆ -90.48      ┆ 15.0   ┆ 4.62 ┆ … ┆ 1.8        ┆ 0.059    ┆ 0.0    ┆ 0.0   │\n",
            "│ 32.998667 ┆ -115.5575   ┆ 14.19  ┆ 5.8  ┆ … ┆ 1.78       ┆ 0.059    ┆ 0.0    ┆ 1.0   │\n",
            "│ 38.19     ┆ -83.95      ┆ 10.0   ┆ 5.0  ┆ … ┆ 1.8        ┆ 0.059    ┆ 29.0   ┆ 0.0   │\n",
            "│ 35.816    ┆ -117.816333 ┆ 4.766  ┆ 4.7  ┆ … ┆ 31.61      ┆ 0.424    ┆ 9.0    ┆ 0.0   │\n",
            "│ 33.0955   ┆ -115.6245   ┆ 18.904 ┆ 5.75 ┆ … ┆ 0.67       ┆ 0.161    ┆ 6.0    ┆ 0.0   │\n",
            "└───────────┴─────────────┴────────┴──────┴───┴────────────┴──────────┴────────┴───────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1.5: Train test split\n",
        "\n",
        "Split the dataset into training and testing sets:\n",
        "- With **30% testing data** and **70% training data**.\n",
        "- Set the **random state** to **2025**.\n",
        "- Use **stratified splitting** to **maintain the same proportion of each class** in the target variable (**`Alert`**) in both the training and testing sets.\n",
        "\n",
        "Display the descriptive statistics for X_train and X_test."
      ],
      "metadata": {
        "id": "mxDVkMQ-hEem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Polars DataFrame to Pandas for scikit-learn compatibility\n",
        "df_pandas = df.to_pandas()\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df_pandas.drop(columns=[\"Alert\"])\n",
        "y = df_pandas[\"Alert\"]\n",
        "\n",
        "# Perform stratified train-test split (70% training, 30% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=2025, stratify=y\n",
        ")\n",
        "\n",
        "# Convert back to Polars DataFrame for analysis\n",
        "X_train_pl = pl.from_pandas(X_train)\n",
        "X_test_pl = pl.from_pandas(X_test)\n",
        "\n",
        "# Display descriptive statistics\n",
        "print(\"Descriptive Statistics for X_train:\")\n",
        "print(X_train_pl.describe())\n",
        "\n",
        "print(\"\\nDescriptive Statistics for X_test:\")\n",
        "print(X_test_pl.describe())"
      ],
      "metadata": {
        "id": "DG9n0DVHwJp0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba4a33c9-3c16-40ff-9090-d54b1c09df2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descriptive Statistics for X_train:\n",
            "shape: (9, 12)\n",
            "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬──────────┬───────────┐\n",
            "│ statistic ┆ latitude  ┆ longitude ┆ depth     ┆ … ┆ horizonta ┆ depthErro ┆ magError ┆ magNst    │\n",
            "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ lError    ┆ r         ┆ ---      ┆ ---       │\n",
            "│ str       ┆ f64       ┆ f64       ┆ f64       ┆   ┆ ---       ┆ ---       ┆ f64      ┆ f64       │\n",
            "│           ┆           ┆           ┆           ┆   ┆ f64       ┆ f64       ┆          ┆           │\n",
            "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪══════════╪═══════════╡\n",
            "│ count     ┆ 5389.0    ┆ 5389.0    ┆ 5389.0    ┆ … ┆ 5389.0    ┆ 5389.0    ┆ 5389.0   ┆ 5389.0    │\n",
            "│ null_coun ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ … ┆ 0.0       ┆ 0.0       ┆ 0.0      ┆ 0.0       │\n",
            "│ t         ┆           ┆           ┆           ┆   ┆           ┆           ┆          ┆           │\n",
            "│ mean      ┆ 1.196909  ┆ 14.828723 ┆ 58.076958 ┆ … ┆ 6.98469   ┆ 2.524978  ┆ 0.065569 ┆ 41.092225 │\n",
            "│ std       ┆ 32.640056 ┆ 128.02419 ┆ 118.04858 ┆ … ┆ 2.648101  ┆ 3.065915  ┆ 0.049294 ┆ 57.690582 │\n",
            "│           ┆           ┆ 9         ┆ 9         ┆   ┆           ┆           ┆          ┆           │\n",
            "│ min       ┆ -69.7739  ┆ -179.9776 ┆ -1.77     ┆ … ┆ 0.08      ┆ 0.0       ┆ 0.0      ┆ 0.0       │\n",
            "│ 25%       ┆ -21.7957  ┆ -110.7745 ┆ 10.0      ┆ … ┆ 5.9       ┆ 1.7       ┆ 0.052    ┆ 21.0      │\n",
            "│ 50%       ┆ -3.3488   ┆ 39.3035   ┆ 13.0      ┆ … ┆ 7.1       ┆ 1.8       ┆ 0.059    ┆ 29.0      │\n",
            "│ 75%       ┆ 29.3592   ┆ 140.6546  ┆ 41.14     ┆ … ┆ 8.21      ┆ 2.8       ┆ 0.068    ┆ 39.0      │\n",
            "│ max       ┆ 85.729    ┆ 179.9604  ┆ 670.81    ┆ … ┆ 88.54     ┆ 31.95     ┆ 1.642    ┆ 884.0     │\n",
            "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴──────────┴───────────┘\n",
            "\n",
            "Descriptive Statistics for X_test:\n",
            "shape: (9, 12)\n",
            "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬──────────┬───────────┐\n",
            "│ statistic ┆ latitude  ┆ longitude ┆ depth     ┆ … ┆ horizonta ┆ depthErro ┆ magError ┆ magNst    │\n",
            "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ lError    ┆ r         ┆ ---      ┆ ---       │\n",
            "│ str       ┆ f64       ┆ f64       ┆ f64       ┆   ┆ ---       ┆ ---       ┆ f64      ┆ f64       │\n",
            "│           ┆           ┆           ┆           ┆   ┆ f64       ┆ f64       ┆          ┆           │\n",
            "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪══════════╪═══════════╡\n",
            "│ count     ┆ 2310.0    ┆ 2310.0    ┆ 2310.0    ┆ … ┆ 2310.0    ┆ 2310.0    ┆ 2310.0   ┆ 2310.0    │\n",
            "│ null_coun ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ … ┆ 0.0       ┆ 0.0       ┆ 0.0      ┆ 0.0       │\n",
            "│ t         ┆           ┆           ┆           ┆   ┆           ┆           ┆          ┆           │\n",
            "│ mean      ┆ 0.53834   ┆ 15.979179 ┆ 61.120117 ┆ … ┆ 7.017238  ┆ 2.611522  ┆ 0.064856 ┆ 43.975325 │\n",
            "│ std       ┆ 31.698853 ┆ 129.07724 ┆ 121.71298 ┆ … ┆ 2.39899   ┆ 3.11102   ┆ 0.047096 ┆ 64.174417 │\n",
            "│           ┆           ┆ 9         ┆ 2         ┆   ┆           ┆           ┆          ┆           │\n",
            "│ min       ┆ -65.552   ┆ -179.9533 ┆ -0.924    ┆ … ┆ 0.08      ┆ 0.0       ┆ 0.0      ┆ 0.0       │\n",
            "│ 25%       ┆ -21.6779  ┆ -108.662  ┆ 10.0      ┆ … ┆ 5.9       ┆ 1.7       ┆ 0.052    ┆ 22.0      │\n",
            "│ 50%       ┆ -3.8727   ┆ 45.0259   ┆ 15.44     ┆ … ┆ 7.1       ┆ 1.8       ┆ 0.059    ┆ 29.0      │\n",
            "│ 75%       ┆ 26.404    ┆ 141.6605  ┆ 43.0      ┆ … ┆ 8.3       ┆ 3.0       ┆ 0.066    ┆ 40.0      │\n",
            "│ max       ┆ 85.2392   ┆ 179.9981  ┆ 639.503   ┆ … ┆ 15.3      ┆ 31.61     ┆ 1.334    ┆ 954.0     │\n",
            "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴──────────┴───────────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Sequential Feature Selection"
      ],
      "metadata": {
        "id": "DkcOJ8fErgu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2.1: Forward Selection\n",
        "\n",
        "Create a pipeline using [`SequentialFeatureSelector`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html) to perform **forward** feature selection:\n",
        "- Standardize the data, ensuring each feature has a mean of 0 and a standard deviation of 1.\n",
        "- Use **ROC AUC** as the scoring metric for feature selection. Conduct **5-fold cross-validation** to evaluate the model. Set the **tolerance for stopping** the selection process to **0.001**.\n",
        "- Configure the logistic regression to use the default **`lbfgs`** as solver with **no penalty**. Set the **maximum number of iterations** to **1000**. Use a **balanced** weight that adjust weights inversely proportional to class frequencies in the input data. Set the **random state** to **2025**.\n",
        "\n",
        "Fit the pipeline and report the subset of variables on this method.\n"
      ],
      "metadata": {
        "id": "6MKT3NRarkz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Standardization step\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Define Logistic Regression Model\n",
        "log_reg = LogisticRegression(\n",
        "    solver=\"lbfgs\",\n",
        "    penalty=None,  # No regularization\n",
        "    max_iter=1000,\n",
        "    class_weight=\"balanced\",  # Adjusts for class imbalance\n",
        "    random_state=2025\n",
        ")\n",
        "\n",
        "# Define Forward Feature Selection using standard cross-validation (not StratifiedKFold)\n",
        "sfs = SequentialFeatureSelector(\n",
        "    log_reg,\n",
        "    n_features_to_select=\"auto\",  # Auto-determines the best number of features\n",
        "    direction=\"forward\",  # Forward selection\n",
        "    scoring=\"roc_auc\",  # Use ROC AUC as metric\n",
        "    cv=5,  # 5-fold cross-validation\n",
        "    tol=0.001,  # Stopping tolerance\n",
        "    n_jobs=-1  # Use all available processors\n",
        ")\n",
        "\n",
        "# Create a pipeline with standardization and feature selection\n",
        "pipeline = Pipeline([\n",
        "    (\"scaler\", scaler),\n",
        "    (\"feature_selection\", sfs),\n",
        "    (\"log_reg\", log_reg)\n",
        "])\n",
        "\n",
        "# Fit the pipeline\n",
        "pipeline.fit(X, y)\n",
        "\n",
        "# Get the selected features\n",
        "selected_features = X.columns[pipeline.named_steps[\"feature_selection\"].get_support()]\n",
        "\n",
        "# Output the selected features\n",
        "print(\"Selected Features:\", selected_features.tolist())"
      ],
      "metadata": {
        "id": "I0gfWRgo1P14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "223118d5-0e0e-4dad-9469-6cd2ce011e13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Features: ['latitude', 'depth', 'mag', 'dmin', 'horizontalError']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2.2: Backward Selection\n",
        "\n",
        "Create a pipeline using [`SequentialFeatureSelector`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html) to perform **backward** feature selection. Keep all other configurations the same as in the previous question\n",
        "\n",
        "Fit the pipeline and report the subset of variables on this method.\n"
      ],
      "metadata": {
        "id": "ACAsAnUdLB_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Backward Feature Selection using the same settings as before\n",
        "sbs = SequentialFeatureSelector(\n",
        "    log_reg,\n",
        "    n_features_to_select=\"auto\",  # Auto-determines the best number of features\n",
        "    direction=\"backward\",  # Backward selection\n",
        "    scoring=\"roc_auc\",  # Use ROC AUC as metric\n",
        "    cv=5,  # 5-fold cross-validation\n",
        "    tol=0.001,  # Stopping tolerance\n",
        "    n_jobs=-1  # Use all available processors\n",
        ")\n",
        "\n",
        "# Create a pipeline with standardization and feature selection\n",
        "pipeline = Pipeline([\n",
        "    (\"scaler\", scaler),\n",
        "    (\"feature_selection\", sbs),\n",
        "    (\"log_reg\", log_reg)\n",
        "])\n",
        "\n",
        "# Fit the pipeline\n",
        "pipeline.fit(X, y)\n",
        "\n",
        "# Get the selected features\n",
        "selected_features = X.columns[pipeline.named_steps[\"feature_selection\"].get_support()]\n",
        "\n",
        "# Output the selected features\n",
        "print(\"Selected Features:\", selected_features.tolist())"
      ],
      "metadata": {
        "id": "fnrafwMxK08Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8035fc13-5098-45c4-c7a3-78bf26ae9ad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Features: ['latitude', 'depth', 'mag', 'gap', 'dmin', 'rms', 'horizontalError', 'depthError', 'magError', 'magNst']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2.3: Compare results & find the best model\n",
        "\n",
        "Compare and discuss the selected subset of variables obtained from both methods used in the previous steps.\n",
        "\n",
        "Perform an **exhaustive search** over all possible subsets of the remaining variables using **5-fold cross-validation** to find the best model. Use the same Logistic Regression configurations as in previous questions.\n",
        "\n",
        "Hint: If you have correctly followed the previous steps, you should have **five remaining variables** to evaluate in the exhaustive search."
      ],
      "metadata": {
        "id": "8cSXKQpXMlwm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Your written answer here)\n",
        "\n",
        "The feature selection methods produced different subsets, highlighting distinct factors influencing earthquake severity classification. Forward selection chose a compact set of five features, focusing on location-based attributes such as latitude, depth, and dmin, as well as magnitude `mag` and `horizontalError`, indicating that positional and measurement accuracy contribute to severity prediction. In contrast, backward selection retained a larger set of ten features, keeping all forward-selected variables while also incorporating measurement uncertainties (`depthError`, `magError`, `rms`) and seismic station-related variables (`gap`, `magNst`), suggesting a more comprehensive but potentially overfitted model. Exhaustive search, which systematically tested all subsets of the five remaining features, identified `gap`, `rms`, `depthError`, `magError`, and `magNst` as the best-performing combination, achieving a ROC AUC score of 0.660. Notably, none of the forward-selected features appeared in this set, implying that measurement errors and sensor quality metrics may be more predictive of earthquake severity than geographic attributes. While forward selection offers a simple, efficient model, it may overlook key uncertainty-based predictors, whereas backward selection retains more information at the risk of overfitting. Exhaustive search provides the most statistically optimal subset among the remaining features, reinforcing the significance of measurement reliability in seismic classification."
      ],
      "metadata": {
        "id": "fEAnA4JUOgw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to generate all possible subsets\n",
        "def get_feature_subsets(features):\n",
        "    return list(chain.from_iterable(combinations(features, r) for r in range(1, len(features) + 1)))\n",
        "\n",
        "# Identify the five remaining features that were not selected by forward or backward selection\n",
        "remaining_features = ['gap', 'rms', 'depthError', 'magError', 'magNst']\n",
        "\n",
        "# Generate all possible subsets\n",
        "feature_subsets = get_feature_subsets(remaining_features)\n",
        "\n",
        "# Initialize variables for tracking the best model\n",
        "best_score = 0\n",
        "best_subset = None\n",
        "\n",
        "# Perform exhaustive search\n",
        "for subset in feature_subsets:\n",
        "    X_subset = X[list(subset)]\n",
        "    X_scaled = StandardScaler().fit_transform(X_subset)\n",
        "\n",
        "    # Evaluate using 5-fold cross-validation with ROC AUC\n",
        "    score = np.mean(cross_val_score(log_reg, X_scaled, y, cv=5, scoring=\"roc_auc\"))\n",
        "\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_subset = subset\n",
        "\n",
        "# Output the best feature subset and corresponding score\n",
        "print(\"Best Feature Subset:\", list(best_subset))\n",
        "print(\"Best ROC AUC Score:\", best_score)"
      ],
      "metadata": {
        "id": "Qjjk41LbMlo5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54be26cf-3a22-4b23-d019-f54a90394c96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Feature Subset: ['gap', 'rms', 'depthError', 'magError', 'magNst']\n",
            "Best ROC AUC Score: 0.660272194430048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2.4: Fit the best model\n",
        "\n",
        "Train a logistic regression model with the best variables selected.\n",
        "\n",
        "Display the model's coefficients and intercept."
      ],
      "metadata": {
        "id": "Qe7q3KFzSCOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train logistic regression model with the best-selected features\n",
        "X_best = X[list(best_subset)]\n",
        "X_scaled = StandardScaler().fit_transform(X_best)\n",
        "log_reg.fit(X_scaled, y)\n",
        "\n",
        "# Display model coefficients and intercept\n",
        "print(\"Coefficients:\", log_reg.coef_)\n",
        "print(\"Intercept:\", log_reg.intercept_)"
      ],
      "metadata": {
        "id": "YdCYh9e3MlPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "049a5d54-db64-4069-992c-ebb4309bcee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [[-0.46609096 -0.22826086  0.25556911 -1.71393251 -0.24877517]]\n",
            "Intercept: [-0.34537647]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2.5: Measure model's performance\n",
        "\n",
        "Construct a **95% confidence interval** for both accuracy and AUC using **100 bootstrap resamples** of the test set."
      ],
      "metadata": {
        "id": "NkqKda5RV0hZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(2025) # DO NOT DELETE\n",
        "\n",
        "# Extract only the best-selected features from the test set\n",
        "X_test_best = X_test[list(best_subset)]\n",
        "\n",
        "# Initialize lists to store bootstrap results\n",
        "accuracy_scores = []\n",
        "auc_scores = []\n",
        "\n",
        "# Number of bootstrap resamples\n",
        "num_bootstrap = 100\n",
        "\n",
        "# Perform bootstrap resampling\n",
        "for _ in range(num_bootstrap):\n",
        "    # Sample test data with replacement\n",
        "    indices = np.random.choice(len(X_test_best), size=len(X_test_best), replace=True)\n",
        "    X_sampled, y_sampled = X_test_best.iloc[indices], y_test.iloc[indices]\n",
        "\n",
        "    # Standardize features using the same scaler as training\n",
        "    X_sampled_scaled = StandardScaler().fit_transform(X_sampled)\n",
        "\n",
        "    # Predict using the trained logistic regression model\n",
        "    y_pred = log_reg.predict(X_sampled_scaled)\n",
        "    y_pred_prob = log_reg.predict_proba(X_sampled_scaled)[:, 1]\n",
        "\n",
        "    # Compute accuracy and AUC for the resampled data\n",
        "    accuracy_scores.append(accuracy_score(y_sampled, y_pred))\n",
        "    auc_scores.append(roc_auc_score(y_sampled, y_pred_prob))\n",
        "\n",
        "# Compute 95% confidence intervals\n",
        "accuracy_ci = np.percentile(accuracy_scores, [2.5, 97.5])\n",
        "auc_ci = np.percentile(auc_scores, [2.5, 97.5])\n",
        "\n",
        "# Display results\n",
        "print(\"95% Confidence Interval for Accuracy:\", accuracy_ci)\n",
        "print(\"95% Confidence Interval for AUC:\", auc_ci)"
      ],
      "metadata": {
        "id": "u7JoX1dnMFUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc48c964-d4c2-44db-9981-e3b40bbb7114"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95% Confidence Interval for Accuracy: [0.51358225 0.60393939]\n",
            "95% Confidence Interval for AUC: [0.63197377 0.72592525]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Regularization"
      ],
      "metadata": {
        "id": "DMFb9isT5HG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3.1: Ridge penalization\n",
        "\n",
        "Create a pipeline using [`LogisticRegressionCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html) to implement a logistic regression with **Ridge penalization**:\n",
        "- Standardize the data, ensuring each feature has a mean of 0 and a standard deviation of 1.\n",
        "- Configure the **`LogisticRegressionCV`** to use **`saga`** as solver. Use the default **`Cs = 10`**. Set the **maximum number of iterations** to **1000**. Use a **balanced** weight that adjust weights inversely proportional to class frequencies in the input data. Set the **random state** to **2025**. Use **5-fold cross-validation**.\n",
        "\n",
        "Fit the pipeline to get the best model.\n",
        "\n",
        "Construct a **95% confidence interval** for both accuracy and AUC using **100 bootstrap resamples** of the test set."
      ],
      "metadata": {
        "id": "UN8KyRvX6D_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(2025) # DO NOT DELETE\n",
        "\n",
        "# Define the pipeline with standardization and LogisticRegressionCV with Ridge penalization\n",
        "pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"log_reg_cv\", LogisticRegressionCV(\n",
        "        solver=\"saga\",\n",
        "        penalty=\"l2\",  # Ridge Penalization\n",
        "        Cs=10,  # Default number of regularization strengths\n",
        "        max_iter=1000,\n",
        "        class_weight=\"balanced\",\n",
        "        random_state=2025,\n",
        "        cv=5  # 5-fold cross-validation\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Fit the pipeline using the best-selected features\n",
        "X_best = X[list(best_subset)]\n",
        "X_scaled = StandardScaler().fit_transform(X_best)\n",
        "pipeline.fit(X_best, y)\n",
        "\n",
        "# Extract the best model from LogisticRegressionCV\n",
        "best_model = pipeline.named_steps[\"log_reg_cv\"]\n",
        "\n",
        "# Initialize lists for bootstrap resampling\n",
        "accuracy_scores = []\n",
        "auc_scores = []\n",
        "\n",
        "# Number of bootstrap resamples\n",
        "num_bootstrap = 100\n",
        "\n",
        "# Perform bootstrap resampling on test set\n",
        "X_test_best = X_test[list(best_subset)]  # Ensure only selected features are used\n",
        "\n",
        "for _ in range(num_bootstrap):\n",
        "    # Sample test data with replacement\n",
        "    indices = np.random.choice(len(X_test_best), size=len(X_test_best), replace=True)\n",
        "    X_sampled, y_sampled = X_test_best.iloc[indices], y_test.iloc[indices]\n",
        "\n",
        "    # Standardize features using the same scaler\n",
        "    X_sampled_scaled = StandardScaler().fit_transform(X_sampled)\n",
        "\n",
        "    # Predict using the best model\n",
        "    y_pred = best_model.predict(X_sampled_scaled)\n",
        "    y_pred_prob = best_model.predict_proba(X_sampled_scaled)[:, 1]\n",
        "\n",
        "    # Compute accuracy and AUC\n",
        "    accuracy_scores.append(accuracy_score(y_sampled, y_pred))\n",
        "    auc_scores.append(roc_auc_score(y_sampled, y_pred_prob))\n",
        "\n",
        "# Compute 95% confidence intervals\n",
        "accuracy_ci = np.percentile(accuracy_scores, [2.5, 97.5])\n",
        "auc_ci = np.percentile(auc_scores, [2.5, 97.5])\n",
        "\n",
        "# Display results\n",
        "print(\"95% Confidence Interval for Accuracy:\", accuracy_ci)\n",
        "print(\"95% Confidence Interval for AUC:\", auc_ci)"
      ],
      "metadata": {
        "id": "QZh1TMq942kd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a427d6ab-aacf-405e-a10d-1916c3005ab2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95% Confidence Interval for Accuracy: [0.51385281 0.60348485]\n",
            "95% Confidence Interval for AUC: [0.63200262 0.72588811]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3.2: Lasso penalization\n",
        "\n",
        "Create a pipeline using [`LogisticRegressionCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html) to implement a logistic regression with **Lasso penalization**:\n",
        "- Standardize the data, ensuring each feature has a mean of 0 and a standard deviation of 1.\n",
        "- Configure the **`LogisticRegressionCV`** to use **`saga`** as solver. Use the default **`Cs = 10`**. Set the **maximum number of iterations** to **1000**. Use a **balanced** weight that adjust weights inversely proportional to class frequencies in the input data. Set the **random state** to **2025**. Use **5-fold cross-validation**.\n",
        "\n",
        "Fit the pipeline to get the best model.\n",
        "\n",
        "Construct a **95% confidence interval** for both accuracy and AUC using **100 bootstrap resamples** of the test set."
      ],
      "metadata": {
        "id": "LWwwyAFxEs3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(2025) # DO NOT DELETE\n",
        "\n",
        "# Define the pipeline with standardization and LogisticRegressionCV with Lasso penalization\n",
        "pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"log_reg_cv\", LogisticRegressionCV(\n",
        "        solver=\"saga\",\n",
        "        penalty=\"l1\",  # Lasso Penalization\n",
        "        Cs=10,  # Default number of regularization strengths\n",
        "        max_iter=1000,\n",
        "        class_weight=\"balanced\",\n",
        "        random_state=2025,\n",
        "        cv=5,  # 5-fold cross-validation\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Fit the pipeline using the best-selected features\n",
        "X_best = X[list(best_subset)]\n",
        "pipeline.fit(X_best, y)\n",
        "\n",
        "for _ in range(num_bootstrap):\n",
        "    # Sample test data with replacement\n",
        "    indices = np.random.choice(len(X_test_best), size=len(X_test_best), replace=True)\n",
        "    X_sampled, y_sampled = X_test_best.iloc[indices], y_test.iloc[indices]\n",
        "\n",
        "    # Standardize features using the same scaler\n",
        "    X_sampled_scaled = pipeline.named_steps[\"scaler\"].transform(X_sampled)\n",
        "\n",
        "    # Predict using the best model\n",
        "    y_pred = best_model.predict(X_sampled_scaled)\n",
        "    y_pred_prob = best_model.predict_proba(X_sampled_scaled)[:, 1]\n",
        "\n",
        "    # Compute accuracy and AUC\n",
        "    accuracy_scores.append(accuracy_score(y_sampled, y_pred))\n",
        "    auc_scores.append(roc_auc_score(y_sampled, y_pred_prob))\n",
        "\n",
        "# Compute 95% confidence intervals\n",
        "accuracy_ci = np.percentile(accuracy_scores, [2.5, 97.5])\n",
        "auc_ci = np.percentile(auc_scores, [2.5, 97.5])\n",
        "\n",
        "# Display results\n",
        "print(\"95% Confidence Interval for Accuracy:\", accuracy_ci)\n",
        "print(\"95% Confidence Interval for AUC:\", auc_ci)"
      ],
      "metadata": {
        "id": "EOFq1EqUDR5F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "204fff44-8eb6-47d8-88ea-26879f11b293"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95% Confidence Interval for Accuracy: [0.51514069 0.59959957]\n",
            "95% Confidence Interval for AUC: [0.63551573 0.72782875]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3.3: Ridge vs Lasso\n",
        "\n",
        "Report the coefficients from the best-performing models with Ridge and Lasso penalties.\n",
        "\n",
        "Compare and discuss how the coefficient magnitudes differ between Ridge and Lasso, and explain."
      ],
      "metadata": {
        "id": "yB9OrR_SFmUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Ridge Model\n",
        "pipeline_ridge = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"log_reg_cv\", LogisticRegressionCV(\n",
        "        solver=\"saga\",\n",
        "        penalty=\"l2\",  # Ridge\n",
        "        Cs=10,\n",
        "        max_iter=1000,\n",
        "        class_weight=\"balanced\",\n",
        "        random_state=2025,\n",
        "        cv=5\n",
        "    ))\n",
        "])\n",
        "pipeline_ridge.fit(X[list(best_subset)], y)\n",
        "\n",
        "# Train Lasso Model\n",
        "pipeline_lasso = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"log_reg_cv\", LogisticRegressionCV(\n",
        "        solver=\"saga\",\n",
        "        penalty=\"l1\",  # Lasso\n",
        "        Cs=10,\n",
        "        max_iter=1000,\n",
        "        class_weight=\"balanced\",\n",
        "        random_state=2025,\n",
        "        cv=5\n",
        "    ))\n",
        "])\n",
        "pipeline_lasso.fit(X[list(best_subset)], y)\n",
        "\n",
        "# Now extract coefficients\n",
        "ridge_coefs = pipeline_ridge.named_steps[\"log_reg_cv\"].coef_\n",
        "lasso_coefs = pipeline_lasso.named_steps[\"log_reg_cv\"].coef_\n",
        "\n",
        "# Compare coefficients in a DataFrame\n",
        "coefs_df = pd.DataFrame({\n",
        "    \"Feature\": list(best_subset),\n",
        "    \"Ridge Coefficients\": ridge_coefs.flatten(),\n",
        "    \"Lasso Coefficients\": lasso_coefs.flatten()\n",
        "})\n",
        "\n",
        "print(coefs_df)"
      ],
      "metadata": {
        "id": "90aHPsKTE6vc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cea0b0c0-da64-4121-d8c3-e2618636c769"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Feature  Ridge Coefficients  Lasso Coefficients\n",
            "0         gap           -0.465544                 0.0\n",
            "1         rms           -0.228122                 0.0\n",
            "2  depthError            0.255290                 0.0\n",
            "3    magError           -1.709444                 0.0\n",
            "4      magNst           -0.247873                 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The coefficient magnitudes differ significantly between Ridge and Lasso due to the way each regularization method handles feature selection and shrinkage. Ridge regression (`L2` regularization) retains all features by shrinking their coefficients towards zero but never eliminating them completely. In this case, Ridge assigned the largest coefficient to `magError (-1.709)`, indicating that uncertainty in magnitude measurement has a strong influence on earthquake severity classification. Other features, such as `gap (-0.466)`, `rms (-0.228)`, `depthError (0.255)`, and `magNst (-0.248)`, were also retained, suggesting they contribute some predictive value. In contrast, Lasso regression (`L1` regularization) applies a sparsity constraint, selectively eliminating features by setting their coefficients to exactly zero (0.0). This indicates that Lasso determined that none of these five features were essential for classification after penalization, likely due to collinearity with other variables or insufficient predictive power. The difference between the two methods highlights how Ridge favors models that distribute information across multiple features, while Lasso acts as an automatic feature selector, discarding less informative or redundant variables to enhance model interpretability."
      ],
      "metadata": {
        "id": "3fgLpuw3ItWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4: Overall Comparison\n",
        "\n",
        "Compare the best models obtained using **Sequential Feature Selection**, **Ridge Regularization**, and **Lasso Regularization** by plotting the ROC curve for each model on a single plot. Additionally, include a diagonal reference line representing random classification performance (i.e., an ROC curve with an AUC of 0.5).\n",
        "\n",
        "Provide a brief analysis and comment on your findings (no need to identify the best model)."
      ],
      "metadata": {
        "id": "DoMw6CaUMx-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n"
      ],
      "metadata": {
        "id": "VI4pxr-mHj8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Your written answer here)\n"
      ],
      "metadata": {
        "id": "rxg6xDcWPzyc"
      }
    }
  ]
}